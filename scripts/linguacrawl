#!/usr/bin/env python

import sys
import logging
import argparse
import signal

from linguacrawl import parse_config
from linguacrawl.multi_site_crawler import MultiSiteCrawler
from linguacrawl.bitext_scout import BitextScout

def main(argv):
    oparser = argparse.ArgumentParser(
        description="Linguacrawl crawls one or more top-level domains looking for documents in a collection languages described by the user."
                    "It is also possible to define a condition about the minimum amount of data in one of the languages; this allows to stop crawling pages that are not promissing.")
    oparser.add_argument("config", metavar="FILE", nargs="?", help="Config file of the crawler", default=None)
    options = oparser.parse_args()
    
    config = parse_config.parse(options.config)

    # If verbose is True, debuging level is set to INFO; otherwise it is ERROR
    logging.basicConfig(level=logging.INFO if config["verbose"] else logging.ERROR)

    
    seed_urls = []
    if "seed_urls" not in config or config["seed_urls"] is None:
        with open(config["seed_urls_from_file"], "r") as reader:
            config["seed_urls"] = []
            for line in reader:
                seed_urls.append(line.strip())
    else:
        for line in config["seed_urls"]:
            seed_urls.append(line)
    
    crawler = MultiSiteCrawler(config, BitextScout(config["scout_steps"], config["langs_of_interest"], config["min_langs_in_site"],
                                                   config["mandatory_lang"], config["min_percent_mandatory_lang"]))
    
    # crawler.add_url_filter('\.(jpg|jpeg|gif|png|js|css|swf)$')
    signal.signal(signal.SIGTERM, crawler.termsighandler)
    signal.signal(signal.SIGINT, crawler.termsighandler)
    crawler.start_crawling()

if __name__=="__main__":
	main(sys.argv[1:])
